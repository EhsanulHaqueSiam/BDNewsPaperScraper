name: Daily News Scraper

# GitHub Free Tier Limits:
# - Public repos: UNLIMITED minutes
# - Private repos: 2,000 minutes/month
# - Max job time: 6 hours
# - Storage: 500MB artifacts (90 days retention)

on:
  schedule:
    # Run daily at 6 AM UTC (12 PM Bangladesh Time)
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      spiders:
        description: 'Comma-separated spiders (empty = top 20 reliable)'
        required: false
        default: ''
      max_pages:
        description: 'Max pages per spider'
        required: false
        default: '5'
      run_all:
        description: 'Run ALL 71 spiders (takes longer)'
        type: boolean
        default: false

permissions:
  contents: write

env:
  PYTHON_VERSION: '3.11'
  TZ: 'Asia/Dhaka'

concurrency:
  group: daily-scrape
  cancel-in-progress: false  # Don't cancel in-progress scrapes

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 120  # 2 hour max to stay within free tier
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 1
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[cloudflare]" --quiet
          pip install pandas openpyxl pyarrow --quiet
      
      - name: Install Playwright browsers
        run: |
          playwright install chromium --with-deps
      
      - name: Get today's date
        id: date
        run: echo "today=$(date +'%Y-%m-%d')" >> $GITHUB_OUTPUT
      
      - name: Create directories
        run: |
          mkdir -p data
          mkdir -p data/database
      
      - name: Load existing database
        continue-on-error: true
        run: |
          if [ -f data/database/news_articles.db ]; then
            cp data/database/news_articles.db news_articles.db
            echo "‚úÖ Loaded existing database"
          fi
      
      - name: Pre-flight Canary Health Check
        id: canary
        continue-on-error: true
        run: |
          echo "üîç Running canary health check..."
          python scripts/canary_check.py --all --format json --output canary_report.json || true
          
          # Check if any healthy spiders exist
          if [ -f canary_report.json ]; then
            HEALTHY=$(python -c "import json; r=json.load(open('canary_report.json')); print(r['summary']['healthy'])")
            TOTAL=$(python -c "import json; r=json.load(open('canary_report.json')); print(r['summary']['total_checked'])")
            echo "canary_healthy=$HEALTHY" >> $GITHUB_OUTPUT
            echo "canary_total=$TOTAL" >> $GITHUB_OUTPUT
            echo "üìä Canary Results: $HEALTHY/$TOTAL healthy"
          else
            echo "canary_healthy=0" >> $GITHUB_OUTPUT
            echo "canary_total=0" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è Canary check failed"
          fi
      
      # Default: Run top 20 most reliable spiders (scheduled or manual without run_all)
      - name: Run top spiders (default)
        if: (github.event_name == 'schedule') || (github.event_name == 'workflow_dispatch' && github.event.inputs.run_all != 'true' && github.event.inputs.spiders == '')
        run: |
          # Top 20 most reliable spiders (API-based + stable HTML)
          SPIDERS=(
            "prothomalo"
            "thedailystar"
            "dailysun"
            "jugantor"
            "tbsnews"
            "ittefaq"
            "unb"
            "bdnews24"
            "dhakatribune"
            "banglatribune"
            "samakal"
            "jagonews24"
            "risingbd"
            "nayadiganta"
            "BDpratidin"
            "manabzamin"
            "dhakapost"
            "barta24"
            "bbcbangla"
            "dwbangla"
          )
          
          TOTAL=${#SPIDERS[@]}
          SUCCESS=0
          FAILED=0
          
          for i in "${!SPIDERS[@]}"; do
            spider="${SPIDERS[$i]}"
            echo ""
            echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
            echo "üï∑Ô∏è [$((i+1))/$TOTAL] $spider"
            echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
            
            if timeout 180 scrapy crawl "$spider" \
              -a start_date=${{ steps.date.outputs.today }} \
              -a end_date=${{ steps.date.outputs.today }} \
              -a max_pages=5 \
              -s LOG_LEVEL=INFO \
              2>&1 | tail -20; then
              SUCCESS=$((SUCCESS+1))
              echo "‚úÖ $spider completed"
            else
              FAILED=$((FAILED+1))
              echo "‚ö†Ô∏è $spider failed/timeout"
            fi
          done
          
          echo ""
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo "üìä Results: $SUCCESS success, $FAILED failed"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
      
      # Manual: Run ALL spiders (only if explicitly requested)
      - name: Run ALL spiders (manual only)
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.run_all == 'true'
        run: |
          SPIDERS=$(scrapy list)
          TOTAL=$(echo "$SPIDERS" | wc -l)
          COUNT=0
          SUCCESS=0
          
          for spider in $SPIDERS; do
            COUNT=$((COUNT+1))
            echo "üï∑Ô∏è [$COUNT/$TOTAL] $spider"
            
            if timeout 120 scrapy crawl "$spider" \
              -a start_date=${{ steps.date.outputs.today }} \
              -a end_date=${{ steps.date.outputs.today }} \
              -a max_pages=${{ github.event.inputs.max_pages }} \
              -s LOG_LEVEL=ERROR \
              2>&1 | tail -3; then
              SUCCESS=$((SUCCESS+1))
            fi
          done
          
          echo "üìä Completed: $SUCCESS/$TOTAL spiders"
      
      # Manual: Run specific spiders
      - name: Run selected spiders
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.spiders != ''
        run: |
          IFS=',' read -ra SPIDERS <<< "${{ github.event.inputs.spiders }}"
          
          for spider in "${SPIDERS[@]}"; do
            spider=$(echo $spider | xargs)
            echo "üï∑Ô∏è Running: $spider"
            
            timeout 180 scrapy crawl "$spider" \
              -a start_date=${{ steps.date.outputs.today }} \
              -a end_date=${{ steps.date.outputs.today }} \
              -a max_pages=${{ github.event.inputs.max_pages }} \
              -s LOG_LEVEL=WARNING \
              2>&1 | tail -10 || echo "‚ö†Ô∏è Failed: $spider"
          done
      
      - name: Export data
        run: |
          python << 'EOF'
          import sqlite3
          import json
          import os
          from datetime import datetime
          
          today = "${{ steps.date.outputs.today }}"
          db_path = "news_articles.db"
          
          if not os.path.exists(db_path):
              print("‚ùå No database found")
              exit(0)
          
          conn = sqlite3.connect(db_path)
          conn.row_factory = sqlite3.Row
          
          # Get today's articles
          cursor = conn.execute('''
              SELECT * FROM articles 
              WHERE date(publication_date) = ? OR date(scraped_at) = ?
              ORDER BY paper_name
          ''', (today, today))
          
          articles = [dict(row) for row in cursor.fetchall()]
          
          if not articles:
              print("üì≠ No articles found for today")
              conn.close()
              exit(0)
          
          # === JSON Export (merge with existing) ===
          json_path = "data/articles.json"
          existing_articles = []
          if os.path.exists(json_path):
              try:
                  with open(json_path, 'r', encoding='utf-8') as f:
                      existing_articles = json.load(f)
                  print(f"üìÇ Loaded {len(existing_articles)} existing articles from JSON")
              except (json.JSONDecodeError, IOError) as e:
                  print(f"‚ö†Ô∏è Could not load existing JSON: {e}")
                  existing_articles = []
          
          # Deduplicate by URL
          existing_urls = {a.get('url') for a in existing_articles if a.get('url')}
          new_articles = [a for a in articles if a.get('url') not in existing_urls]
          all_articles = existing_articles + new_articles
          
          with open(json_path, 'w', encoding='utf-8') as f:
              json.dump(all_articles, f, ensure_ascii=False, indent=2, default=str)
          print(f"üìÑ JSON: {len(new_articles)} new articles added (total: {len(all_articles)})")
          
          # === CSV Export ===
          import pandas as pd
          df = pd.DataFrame(all_articles)
          df.to_csv('data/articles.csv', index=False, encoding='utf-8')
          print(f"üìã CSV: {len(all_articles)} articles exported")
          
          # === Excel Export ===
          df.to_excel('data/articles.xlsx', index=False, engine='openpyxl')
          print(f"üìä XLSX: {len(all_articles)} articles exported")
          
          # === Parquet Export (compressed) ===
          df.to_parquet('data/articles.parquet', compression='snappy', index=False)
          print(f"üóúÔ∏è Parquet: {len(all_articles)} articles exported")
          
          # === Generate cumulative summary README ===
          by_paper = {}
          for a in all_articles:
              paper = a.get('paper_name', 'Unknown')
              by_paper[paper] = by_paper.get(paper, 0) + 1
          
          total_db = conn.execute('SELECT COUNT(*) FROM articles').fetchone()[0]
          
          readme_path = "data/README.md"
          with open(readme_path, 'w') as f:
              f.write(f"# üì∞ Bangladesh News Archive\n\n")
              f.write(f"**Last updated:** {today}\n\n")
              f.write(f"| Metric | Count |\n|--------|-------|\n")
              f.write(f"| Total Articles (JSON) | **{len(all_articles)}** |\n")
              f.write(f"| Total in Database | {total_db} |\n")
              f.write(f"| Added Today | {len(new_articles)} |\n\n")
              f.write(f"## Articles by Newspaper\n\n")
              f.write(f"| Paper | Count |\n|-------|-------|\n")
              for p, c in sorted(by_paper.items(), key=lambda x: -x[1]):
                  f.write(f"| {p} | {c} |\n")
              f.write(f"\n## Files\n\n")
              f.write(f"| Format | File | Description |\n")
              f.write(f"|--------|------|-------------|\n")
              f.write(f"| JSON | `articles.json` | Web apps, JavaScript |\n")
              f.write(f"| CSV | `articles.csv` | Universal, Excel/Sheets |\n")
              f.write(f"| Excel | `articles.xlsx` | Native Excel format |\n")
              f.write(f"| Parquet | `articles.parquet` | Data science, analytics |\n")
              f.write(f"| SQLite | `database/news_articles.db` | Local querying |\n")
          
          print(f"üìä README updated")
          conn.close()
          EOF
      
      - name: Save database
        run: |
          if [ -f news_articles.db ]; then
            cp news_articles.db data/database/news_articles.db
            echo "üíæ Database saved"
          fi
      
      - name: Commit changes
        run: |
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          
          git add data/
          
          if git diff --staged --quiet; then
            echo "No changes"
          else
            git commit -m "üì∞ ${{ steps.date.outputs.today }}: News update"
            git push
          fi
      
      - name: Upload to Kaggle Dataset
        if: success() && hashFiles('data/articles.json') != ''
        env:
          KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          pip install kaggle --quiet
          
          # Create kaggle config
          mkdir -p ~/.kaggle
          echo "{\"username\":\"$KAGGLE_USERNAME\",\"key\":\"$KAGGLE_KEY\"}" > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json
          
          # Update dataset-metadata.json with correct username
          cd data
          sed -i "s/KAGGLE_USERNAME/$KAGGLE_USERNAME/g" dataset-metadata.json || true
          
          # Add id field if missing (required by Kaggle API)
          python3 << 'PYEOF'
          import json
          import os
          
          meta_path = "dataset-metadata.json"
          username = os.environ.get("KAGGLE_USERNAME", "")
          
          with open(meta_path, "r") as f:
              meta = json.load(f)
          
          # Ensure id field exists
          if "id" not in meta:
              meta["id"] = f"{username}/bangladesh-news-articles"
          else:
              meta["id"] = f"{username}/bangladesh-news-articles"
          
          with open(meta_path, "w") as f:
              json.dump(meta, f, indent=2)
          
          print(f"‚úÖ Metadata updated for {meta['id']}")
          PYEOF
          
          # Try to update existing dataset, create if doesn't exist
          if kaggle datasets status "$KAGGLE_USERNAME/bangladesh-news-articles" 2>/dev/null; then
            echo "üì§ Updating existing dataset..."
            kaggle datasets version -p . -m "Daily update: ${{ steps.date.outputs.today }}" --dir-mode zip
          else
            echo "üì§ Creating new dataset..."
            kaggle datasets create -p . --dir-mode zip
          fi
          
          echo "‚úÖ Kaggle upload complete"
          echo "üìä Dataset: https://www.kaggle.com/datasets/$KAGGLE_USERNAME/bangladesh-news-articles"
      
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: news-${{ steps.date.outputs.today }}
          path: |
            data/articles.json
            data/articles.csv
            data/articles.xlsx
            data/articles.parquet
            data/README.md
            data/database/news_articles.db
          retention-days: 30
          compression-level: 9
      
      - name: Summary
        if: always()
        run: |
          echo "## üì∞ Scraping Complete" >> $GITHUB_STEP_SUMMARY
          if [ -f data/README.md ]; then
            cat data/README.md >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üì§ Kaggle Dataset" >> $GITHUB_STEP_SUMMARY
          echo "Dataset: [${KAGGLE_USERNAME}/bangladesh-news-articles](https://www.kaggle.com/datasets/${KAGGLE_USERNAME}/bangladesh-news-articles)" >> $GITHUB_STEP_SUMMARY

